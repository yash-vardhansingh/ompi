.\" Man page generated from reStructuredText.
.
.TH "MPI_PUBLISH_NAME" "3" "Jul 18, 2024" "" "Open MPI"
.
.nr rst2man-indent-level 0
.
.de1 rstReportMargin
\\$1 \\n[an-margin]
level \\n[rst2man-indent-level]
level margin: \\n[rst2man-indent\\n[rst2man-indent-level]]
-
\\n[rst2man-indent0]
\\n[rst2man-indent1]
\\n[rst2man-indent2]
..
.de1 INDENT
.\" .rstReportMargin pre:
. RS \\$1
. nr rst2man-indent\\n[rst2man-indent-level] \\n[an-margin]
. nr rst2man-indent-level +1
.\" .rstReportMargin post:
..
.de UNINDENT
. RE
.\" indent \\n[an-margin]
.\" old: \\n[rst2man-indent\\n[rst2man-indent-level]]
.nr rst2man-indent-level -1
.\" new: \\n[rst2man-indent\\n[rst2man-indent-level]]
.in \\n[rst2man-indent\\n[rst2man-indent-level]]u
..
.sp
\fI\%MPI_Publish_name\fP — Publishes a service name associated with a port
.SH SYNTAX
.SS C Syntax
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
#include <mpi.h>

int MPI_Publish_name(const char *service_name, MPI_Info info,
     const char *port_name)
.ft P
.fi
.UNINDENT
.UNINDENT
.SS Fortran Syntax
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
USE MPI
! or the older form: INCLUDE \(aqmpif.h\(aq
MPI_PUBLISH_NAME(SERVICE_NAME, INFO, PORT_NAME, IERROR)
     CHARACTER*(*)   SERVICE_NAME, PORT_NAME
     INTEGER         INFO, IERROR
.ft P
.fi
.UNINDENT
.UNINDENT
.SS Fortran 2008 Syntax
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
USE mpi_f08
MPI_Publish_name(service_name, info, port_name, ierror)
     TYPE(MPI_Info), INTENT(IN) :: info
     CHARACTER(LEN=*), INTENT(IN) :: service_name, port_name
     INTEGER, OPTIONAL, INTENT(OUT) :: ierror
.ft P
.fi
.UNINDENT
.UNINDENT
.SH INPUT PARAMETERS
.INDENT 0.0
.IP \(bu 2
\fBservice_name\fP: A service name (string).
.IP \(bu 2
\fBinfo\fP: Options to the name service functions (handle).
.IP \(bu 2
\fBport_name\fP: A port name (string).
.UNINDENT
.SH OUTPUT PARAMETER
.INDENT 0.0
.IP \(bu 2
\fBierror\fP: Fortran only: Error status (integer).
.UNINDENT
.SH DESCRIPTION
.sp
This routine publishes the pair (\fIservice_name, port_name\fP) so that an
application may retrieve \fIport_name\fP by calling \fI\%MPI_Lookup_name\fP with
\fIservice_name\fP as an argument. It is an error to publish the same
\fIservice_name\fP twice, or to use a \fIport_name\fP argument that was not
previously opened by the calling process via a call to \fI\%MPI_Open_port\fP\&.
.SH INFO ARGUMENTS
.sp
The following keys for \fIinfo\fP are recognized:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
Key                   Type      Description
\-\-\-                   \-\-\-\-      \-\-\-\-\-\-\-\-\-\-\-

ompi_global_scope     bool      If set to true, publish the name in
                                the global scope.  Publish in the local
                                scope otherwise.  See the NAME SCOPE
                                section for more details.

ompi_unique           bool      If set to true, return an error if the
                                specified service_name already exists.
                                Default to overwriting any pre\-existing
                                value.
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
\fIbool\fP info keys are actually strings but are evaluated as follows: if
the string value is a number, it is converted to an integer and cast to
a boolean (meaning that zero integers are false and non\-zero values are
true). If the string value is (case\-insensitive) “yes” or “true”, the
boolean is true. If the string value is (case\-insensitive) “no” or
“false”, the boolean is false. All other string values are unrecognized,
and therefore false.
.sp
If no info key is provided, the function will first check to see if a
global server has been specified and is available. If so, then the
publish function will default to global scope first, followed by local.
Otherwise, the data will default to publish with local scope.
.SH NAME SCOPE
.sp
Open MPI supports two name scopes: \fIglobal\fP and \fIlocal\fP\&. Local scope
will place the specified service/port pair in a data store located on
the mpirun of the calling process’ job. Thus, data published with local
scope will only be accessible to processes in jobs spawned by that
mpirun \- e.g., processes in the calling process’ job, or in jobs spawned
via \fI\%MPI_Comm_spawn\fP\&.
.sp
Global scope places the specified service/port pair in a data store
located on a central server that is accessible to all jobs running in
the cluster or environment. Thus, data published with global scope can
be accessed by multiple mpiruns and used for \fI\%MPI_Comm_connect\fP and
\fI\%MPI_Comm_accept\fP between jobs.
.sp
Note that global scope operations require both the presence of the
central server and that the calling process be able to communicate to
that server. \fI\%MPI_Publish_name\fP will return an error if global scope is
specified and a global server is either not specified or cannot be
found.
.sp
Open MPI provides a server called \fIompi\-server\fP to support global scope
operations. Please refer to its manual page for a more detailed
description of data store/lookup operations.
.sp
As an example of the impact of these scoping rules, consider the case
where a job has been started with mpirun \- call this job “job1”. A
process in job1 creates and publishes a service/port pair using a local
scope. Open MPI will store this data in the data store within mpirun.
.sp
A process in job1 (perhaps the same as did the publish, or perhaps some
other process in the job) subsequently calls \fI\%MPI_Comm_spawn\fP to start
another job (call it “job2”) under this mpirun. Since the two jobs share
a common mpirun, both jobs have access to local scope data. Hence, a
process in job2 can perform an \fI\%MPI_Lookup_name\fP with a local scope to
retrieve the information.
.sp
However, assume another user starts a job using mpirun \- call this job
“job3”. Because the service/port data published by job1 specified local
scope, processes in job3 cannot access that data. In contrast, if the
data had been published using global scope, then any process in job3
could access the data, provided that mpirun was given knowledge of how
to contact the central server and the process could establish
communication with it.
.SH ERRORS
.sp
Almost all MPI routines return an error value; C routines as the return result
of the function and Fortran routines in the last argument.
.sp
Before the error value is returned, the current MPI error handler associated
with the communication object (e.g., communicator, window, file) is called.
If no communication object is associated with the MPI call, then the call is
considered attached to MPI_COMM_SELF and will call the associated MPI error
handler. When MPI_COMM_SELF is not initialized (i.e., before
\fI\%MPI_Init\fP/\fI\%MPI_Init_thread\fP, after \fI\%MPI_Finalize\fP, or when using the Sessions
Model exclusively) the error raises the initial error handler. The initial
error handler can be changed by calling \fI\%MPI_Comm_set_errhandler\fP on
MPI_COMM_SELF when using the World model, or the mpi_initial_errhandler CLI
argument to mpiexec or info key to \fI\%MPI_Comm_spawn\fP/\fI\%MPI_Comm_spawn_multiple\fP\&.
If no other appropriate error handler has been set, then the MPI_ERRORS_RETURN
error handler is called for MPI I/O functions and the MPI_ERRORS_ABORT error
handler is called for all other MPI functions.
.sp
Open MPI includes three predefined error handlers that can be used:
.INDENT 0.0
.IP \(bu 2
\fBMPI_ERRORS_ARE_FATAL\fP
Causes the program to abort all connected MPI processes.
.IP \(bu 2
\fBMPI_ERRORS_ABORT\fP
An error handler that can be invoked on a communicator,
window, file, or session. When called on a communicator, it
acts as if \fI\%MPI_Abort\fP was called on that communicator. If
called on a window or file, acts as if \fI\%MPI_Abort\fP was called
on a communicator containing the group of processes in the
corresponding window or file. If called on a session,
aborts only the local process.
.IP \(bu 2
\fBMPI_ERRORS_RETURN\fP
Returns an error code to the application.
.UNINDENT
.sp
MPI applications can also implement their own error handlers by calling:
.INDENT 0.0
.IP \(bu 2
\fI\%MPI_Comm_create_errhandler\fP then \fI\%MPI_Comm_set_errhandler\fP
.IP \(bu 2
\fI\%MPI_File_create_errhandler\fP then \fI\%MPI_File_set_errhandler\fP
.IP \(bu 2
\fI\%MPI_Session_create_errhandler\fP then \fI\%MPI_Session_set_errhandler\fP or at \fI\%MPI_Session_init\fP
.IP \(bu 2
\fI\%MPI_Win_create_errhandler\fP then \fI\%MPI_Win_set_errhandler\fP
.UNINDENT
.sp
Note that MPI does not guarantee that an MPI program can continue past
an error.
.sp
See the \fI\%MPI man page\fP for a full list of \fI\%MPI error codes\fP\&.
.sp
See the Error Handling section of the MPI\-3.1 standard for
more information.
.sp
\fBSEE ALSO:\fP
.INDENT 0.0
.INDENT 3.5
.INDENT 0.0
.IP \(bu 2
\fI\%MPI_Lookup_name\fP
.IP \(bu 2
\fI\%MPI_Open_port\fP
.UNINDENT
.UNINDENT
.UNINDENT
.SH COPYRIGHT
2003-2024, The Open MPI Community
.\" Generated by docutils manpage writer.
.
